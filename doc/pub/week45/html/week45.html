<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week45.do.txt --pygments_html_style=default --html_style=bloodish --html_links_in_new_window --html_output=week45 --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 45,  Recurrent Neural Networks">
<title>Week 45,  Recurrent Neural Networks</title>
<style type="text/css">
/* bloodish style */
body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em; color: #8A0808; }
h2 { font-size: 1.6em; color: #8A0808; }
h3 { font-size: 1.4em; color: #8A0808; }
h4 { font-size: 1.2em; color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa; }div.highlight {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    line-height: 1.21429em;
}
div.cell {
    width: 100%;
    padding: 5px 5px 5px 0;
    margin: 0;
    outline: none;
}
div.input {
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.inner_cell {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
    box-flex: 1;
    flex: 1;
}
div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    background: #f7f7f7;
    line-height: 1.21429em;
}
div.input_area > div.highlight {
    margin: .4em;
    border: none;
    padding: 0;
    background-color: transparent;
}
div.output_wrapper {
    position: relative;
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
.output {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
div.output_area {
    padding: 0;
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.output_subarea {
    padding: .4em .4em 0 .4em;
    box-flex: 1;
    flex: 1;
}
div.output_text {
    text-align: left;
    color: #000;
    line-height: 1.21429em;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_question.png); }
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 45', 2, None, 'plan-for-week-45'),
              ('Material for the lab sessions, additional ways to present '
               'classification results and other practicalities',
               2,
               None,
               'material-for-the-lab-sessions-additional-ways-to-present-classification-results-and-other-practicalities'),
              ('Searching for Optimal Regularization Parameters $\\lambda$',
               2,
               None,
               'searching-for-optimal-regularization-parameters-lambda'),
              ('Grid Search', 2, None, 'grid-search'),
              ('Randomized Grid Search', 2, None, 'randomized-grid-search'),
              ('Wisconsin Cancer Data', 2, None, 'wisconsin-cancer-data'),
              ('Using the correlation matrix',
               2,
               None,
               'using-the-correlation-matrix'),
              ('Discussing the correlation data',
               2,
               None,
               'discussing-the-correlation-data'),
              ('Other ways of presenting a classification problem',
               2,
               None,
               'other-ways-of-presenting-a-classification-problem'),
              ('Combinations of classification results',
               2,
               None,
               'combinations-of-classification-results'),
              ('Positive and negative prediction values',
               2,
               None,
               'positive-and-negative-prediction-values'),
              ('Other quantities', 2, None, 'other-quantities'),
              ('$F_1$ score', 2, None, 'f-1-score'),
              ('ROC curve', 2, None, 'roc-curve'),
              ('Cumulative gain curve', 2, None, 'cumulative-gain-curve'),
              ('Other measures in classification studies: Cancer Data  again',
               2,
               None,
               'other-measures-in-classification-studies-cancer-data-again'),
              ('Material for Lecture Thursday November 9',
               2,
               None,
               'material-for-lecture-thursday-november-9'),
              ('Recurrent neural networks (RNNs): Overarching view',
               2,
               None,
               'recurrent-neural-networks-rnns-overarching-view'),
              ('A simple example', 2, None, 'a-simple-example'),
              ('RNNs', 3, None, 'rnns'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('We need to specify the initial activity state of all the '
               'hidden and output units',
               3,
               None,
               'we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units'),
              ('We can specify inputs in several ways',
               3,
               None,
               'we-can-specify-inputs-in-several-ways'),
              ('We can specify targets in several ways',
               3,
               None,
               'we-can-specify-targets-in-several-ways'),
              ('Backpropagation through time',
               3,
               None,
               'backpropagation-through-time'),
              ('The backward pass is linear',
               3,
               None,
               'the-backward-pass-is-linear'),
              ('The problem of exploding or vanishing gradients',
               2,
               None,
               'the-problem-of-exploding-or-vanishing-gradients'),
              ('Four effective ways to learn an RNN',
               2,
               None,
               'four-effective-ways-to-learn-an-rnn'),
              ('Long Short Term Memory (LSTM)',
               3,
               None,
               'long-short-term-memory-lstm'),
              ('Implementing a memory cell in a neural network',
               3,
               None,
               'implementing-a-memory-cell-in-a-neural-network'),
              ('An extrapolation example', 2, None, 'an-extrapolation-example'),
              ('Formatting the Data', 2, None, 'formatting-the-data'),
              ('Predicting New Points With A Trained Recurrent Neural Network',
               2,
               None,
               'predicting-new-points-with-a-trained-recurrent-neural-network'),
              ('Other Things to Try', 2, None, 'other-things-to-try'),
              ('Other Types of Recurrent Neural Networks',
               2,
               None,
               'other-types-of-recurrent-neural-networks')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Week 45,  Recurrent Neural Networks</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>
<!-- institution(s) -->
<center>
[1] <b>Department of Physics, University of Oslo</b>
</center>
<center>
[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b>
</center>
<br>
<center>
<h4>November 6-10</h4>
</center> <!-- date -->
<br>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="plan-for-week-45">Plan for week 45 </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Material for the active learning sessions on Tuesday and Wednesday</b>
<p>
<ul>
  <li> Discussion of project 2</li>
  <li> <a href="https://youtu.be/Ia6wwDLxqtM" target="_blank">Video of lab session from week 43</a></li>
  <li> <a href="https://youtu.be/EajWMW__k0I" target="_blank">Video of lab session from week 44</a></li>
  <li> <a href="https://youtu.be/tgkj0KAEtZo" target="_blank">Video of lab session from week 45</a></li>
  <li> <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2023/Exercisesweek44.pdf" target="_blank">See also whiteboard notes from lab session week 44</a></li>
</ul>
</div>
  

<div class="alert alert-block alert-block alert-text-normal">
<b>Material for the lecture on Thursday November 9, 2023</b>
<p>
<ul>
  <li> Short repetition on Convolutional Neural Networks</li>
  <li> Recurrent  Neural Networks (RNNs)</li>
  <li> Readings and Videos:</li>
<ul>
    <li> These lecture notes</li>
    <li> <a href="https://youtu.be/z0x-vgyAZUk" target="_blank">Video of lecture</a></li>
    <li> <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2023/NotesNov9.pdf" target="_blank">Whiteboard notes</a></li>
    <li> For a more in depth discussion on  neural networks we recommend Goodfellow et al chapter 10. See also chapter 11 and 12 on practicalities and applications</li>    
<ul>
     <li> Reading suggestions for implementation of RNNs: <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/Textbooks/TensorflowML.pdf" target="_blank">Aurelien Geron's chapter 14</a>.</li>
</ul>
    <li> <a href="https://www.youtube.com/watch?v=SEnXr6v2ifU&ab_channel=AlexanderAmini" target="_blank">Video  on Recurrent Neural Networks from MIT</a></li>
    <li> <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" target="_blank">Video on Deep Learning</a></li>
</ul>
</ul>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="material-for-the-lab-sessions-additional-ways-to-present-classification-results-and-other-practicalities">Material for the lab sessions, additional ways to present classification results and other practicalities </h2>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="searching-for-optimal-regularization-parameters-lambda">Searching for Optimal Regularization Parameters \( \lambda \) </h2>

<p>In project 1, when using Ridge and Lasso regression, we end up
searching for the optimal parameter \( \lambda \) which minimizes our
selected scores (MSE or \( R2 \) values for example). The brute force
approach, as discussed in the code here for Ridge regression, consists
in evaluating the MSE as function of different \( \lambda \) values.
Based on these calculations, one tries then to determine the value of the hyperparameter \( \lambda \)
which results in optimal scores (for example the smallest MSE or an \( R2=1 \)).
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn</span> <span style="color: #008000; font-weight: bold">import</span> linear_model

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n
<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2021</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(n)

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">5</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n,Maxpolydegree<span style="color: #666666">-1</span>))

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,Maxpolydegree): <span style="color: #408080; font-style: italic">#No intercept column</span>
    X[:,degree<span style="color: #666666">-1</span>] <span style="color: #666666">=</span> x<span style="color: #666666">**</span>(degree)

<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)

<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">500</span>
MSERidgePredict <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(nlambdas)
lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">2</span>, nlambdas)
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(nlambdas):
    lmb <span style="color: #666666">=</span> lambdas[i]
    RegRidge <span style="color: #666666">=</span> linear_model<span style="color: #666666">.</span>Ridge(lmb)
    RegRidge<span style="color: #666666">.</span>fit(X_train,y_train)
    ypredictRidge <span style="color: #666666">=</span> RegRidge<span style="color: #666666">.</span>predict(X_test)
    MSERidgePredict[i] <span style="color: #666666">=</span> MSE(y_test,ypredictRidge)

<span style="color: #408080; font-style: italic"># Now plot the results</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>plot(np<span style="color: #666666">.</span>log10(lambdas), MSERidgePredict, <span style="color: #BA2121">&#39;g--&#39;</span>, label <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;MSE SL Ridge Test&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;log10(lambda)&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;MSE&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Here we have performed a rather data greedy calculation as function of the regularization parameter \( \lambda \). There is no resampling here. The latter can easily be added by employing the function <b>RidgeCV</b> instead of just calling the <b>Ridge</b> function. For <b>RidgeCV</b> we need to pass the array of \( \lambda \) values.
By inspecting the figure we can in turn determine which is the optimal regularization parameter.
This becomes however less functional in the long run. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="grid-search">Grid Search </h2>

<p>An alternative is to use the so-called grid search functionality
included with the library <b>Scikit-Learn</b>, as demonstrated for the same
example here.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> Ridge
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> GridSearchCV

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n

<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2021</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(n)

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">5</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n,Maxpolydegree<span style="color: #666666">-1</span>))

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,Maxpolydegree): <span style="color: #408080; font-style: italic">#No intercept column</span>
    X[:,degree<span style="color: #666666">-1</span>] <span style="color: #666666">=</span> x<span style="color: #666666">**</span>(degree)

<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)

<span style="color: #408080; font-style: italic"># Decide which values of lambda to use</span>
nlambdas <span style="color: #666666">=</span> <span style="color: #666666">10</span>
lambdas <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-4</span>, <span style="color: #666666">2</span>, nlambdas)
<span style="color: #408080; font-style: italic"># create and fit a ridge regression model, testing each alpha</span>
model <span style="color: #666666">=</span> Ridge()
gridsearch <span style="color: #666666">=</span> GridSearchCV(estimator<span style="color: #666666">=</span>model, param_grid<span style="color: #666666">=</span><span style="color: #008000">dict</span>(alpha<span style="color: #666666">=</span>lambdas))
gridsearch<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(gridsearch)
ypredictRidge <span style="color: #666666">=</span> gridsearch<span style="color: #666666">.</span>predict(X_test)
<span style="color: #408080; font-style: italic"># summarize the results of the grid search</span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Best estimated lambda-value: </span><span style="color: #BB6688; font-weight: bold">{</span>gridsearch<span style="color: #666666">.</span>best_estimator_<span style="color: #666666">.</span>alpha<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;MSE score: </span><span style="color: #BB6688; font-weight: bold">{</span>MSE(y_test,ypredictRidge)<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;R2 score: </span><span style="color: #BB6688; font-weight: bold">{</span>R2(y_test,ypredictRidge)<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>By default the grid search function includes cross validation with
five folds. The <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" target="_blank">Scikit-Learn
documentation</a>
contains more information on how to set the different parameters.
</p>

<p>If we take out the random noise, running the above codes results in \( \lambda=0 \) yielding the best fit. </p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="randomized-grid-search">Randomized Grid Search </h2>

<p>An alternative to the above manual grid set up, is to use a random
search where the parameters are tuned from a random distribution
(uniform below) for a fixed number of iterations. A model is
constructed and evaluated for each combination of chosen parameters.
We repeat the previous example but now with a random search.  Note
that values of \( \lambda \) are now limited to be within \( x\in
[0,1] \). This domain may not be the most relevant one for the specific
case under study.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> Ridge
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> GridSearchCV
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">scipy.stats</span> <span style="color: #008000; font-weight: bold">import</span> uniform <span style="color: #008000; font-weight: bold">as</span> randuniform
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> RandomizedSearchCV


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">R2</span>(y_data, y_model):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> y_model) <span style="color: #666666">**</span> <span style="color: #666666">2</span>) <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum((y_data <span style="color: #666666">-</span> np<span style="color: #666666">.</span>mean(y_data)) <span style="color: #666666">**</span> <span style="color: #666666">2</span>)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MSE</span>(y_data,y_model):
    n <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(y_model)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>sum((y_data<span style="color: #666666">-</span>y_model)<span style="color: #666666">**2</span>)<span style="color: #666666">/</span>n

<span style="color: #408080; font-style: italic"># A seed just to ensure that the random numbers are the same for every run.</span>
<span style="color: #408080; font-style: italic"># Useful for eventual debugging.</span>
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">2021</span>)

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>x<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> <span style="color: #666666">1.5</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(x<span style="color: #666666">-2</span>)<span style="color: #666666">**2</span>)<span style="color: #666666">+</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(n)

Maxpolydegree <span style="color: #666666">=</span> <span style="color: #666666">5</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n,Maxpolydegree<span style="color: #666666">-1</span>))

<span style="color: #008000; font-weight: bold">for</span> degree <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>,Maxpolydegree): <span style="color: #408080; font-style: italic">#No intercept column</span>
    X[:,degree<span style="color: #666666">-1</span>] <span style="color: #666666">=</span> x<span style="color: #666666">**</span>(degree)

<span style="color: #408080; font-style: italic"># We split the data in test and training data</span>
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, test_size<span style="color: #666666">=0.2</span>)

param_grid <span style="color: #666666">=</span> {<span style="color: #BA2121">&#39;alpha&#39;</span>: randuniform()}
<span style="color: #408080; font-style: italic"># create and fit a ridge regression model, testing each alpha</span>
model <span style="color: #666666">=</span> Ridge()
gridsearch <span style="color: #666666">=</span> RandomizedSearchCV(estimator<span style="color: #666666">=</span>model, param_distributions<span style="color: #666666">=</span>param_grid, n_iter<span style="color: #666666">=100</span>)
gridsearch<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(gridsearch)
ypredictRidge <span style="color: #666666">=</span> gridsearch<span style="color: #666666">.</span>predict(X_test)
<span style="color: #408080; font-style: italic"># summarize the results of the grid search</span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Best estimated lambda-value: </span><span style="color: #BB6688; font-weight: bold">{</span>gridsearch<span style="color: #666666">.</span>best_estimator_<span style="color: #666666">.</span>alpha<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;MSE score: </span><span style="color: #BB6688; font-weight: bold">{</span>MSE(y_test,ypredictRidge)<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;R2 score: </span><span style="color: #BB6688; font-weight: bold">{</span>R2(y_test,ypredictRidge)<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="wisconsin-cancer-data">Wisconsin Cancer Data  </h2>

<p>We show here how we can use a simple regression case on the breast
cancer data using Logistic regression as our algorithm for
classification.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression

<span style="color: #408080; font-style: italic"># Load the data</span>
cancer <span style="color: #666666">=</span> load_breast_cancer()

X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(cancer<span style="color: #666666">.</span>data,cancer<span style="color: #666666">.</span>target,random_state<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(X_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(X_test<span style="color: #666666">.</span>shape)
<span style="color: #408080; font-style: italic"># Logistic Regression</span>
logreg <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&#39;lbfgs&#39;</span>)
logreg<span style="color: #666666">.</span>fit(X_train, y_train)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Logistic Regression: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test,y_test)))
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="using-the-correlation-matrix">Using the correlation matrix </h2>

<p>In addition to the above scores, we could also study the covariance (and the correlation matrix).
We use <b>Pandas</b> to compute the correlation matrix.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression
cancer <span style="color: #666666">=</span> load_breast_cancer()
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #408080; font-style: italic"># Making a data frame</span>
cancerpd <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(cancer<span style="color: #666666">.</span>data, columns<span style="color: #666666">=</span>cancer<span style="color: #666666">.</span>feature_names)

fig, axes <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots(<span style="color: #666666">15</span>,<span style="color: #666666">2</span>,figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">20</span>))
malignant <span style="color: #666666">=</span> cancer<span style="color: #666666">.</span>data[cancer<span style="color: #666666">.</span>target <span style="color: #666666">==</span> <span style="color: #666666">0</span>]
benign <span style="color: #666666">=</span> cancer<span style="color: #666666">.</span>data[cancer<span style="color: #666666">.</span>target <span style="color: #666666">==</span> <span style="color: #666666">1</span>]
ax <span style="color: #666666">=</span> axes<span style="color: #666666">.</span>ravel()

<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">30</span>):
    _, bins <span style="color: #666666">=</span> np<span style="color: #666666">.</span>histogram(cancer<span style="color: #666666">.</span>data[:,i], bins <span style="color: #666666">=50</span>)
    ax[i]<span style="color: #666666">.</span>hist(malignant[:,i], bins <span style="color: #666666">=</span> bins, alpha <span style="color: #666666">=</span> <span style="color: #666666">0.5</span>)
    ax[i]<span style="color: #666666">.</span>hist(benign[:,i], bins <span style="color: #666666">=</span> bins, alpha <span style="color: #666666">=</span> <span style="color: #666666">0.5</span>)
    ax[i]<span style="color: #666666">.</span>set_title(cancer<span style="color: #666666">.</span>feature_names[i])
    ax[i]<span style="color: #666666">.</span>set_yticks(())
ax[<span style="color: #666666">0</span>]<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&quot;Feature magnitude&quot;</span>)
ax[<span style="color: #666666">0</span>]<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&quot;Frequency&quot;</span>)
ax[<span style="color: #666666">0</span>]<span style="color: #666666">.</span>legend([<span style="color: #BA2121">&quot;Malignant&quot;</span>, <span style="color: #BA2121">&quot;Benign&quot;</span>], loc <span style="color: #666666">=</span><span style="color: #BA2121">&quot;best&quot;</span>)
fig<span style="color: #666666">.</span>tight_layout()
plt<span style="color: #666666">.</span>show()

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">seaborn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">sns</span>
correlation_matrix <span style="color: #666666">=</span> cancerpd<span style="color: #666666">.</span>corr()<span style="color: #666666">.</span>round(<span style="color: #666666">1</span>)
<span style="color: #408080; font-style: italic"># use the heatmap function from seaborn to plot the correlation matrix</span>
<span style="color: #408080; font-style: italic"># annot = True to print the values inside the square</span>
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">15</span>,<span style="color: #666666">8</span>))
sns<span style="color: #666666">.</span>heatmap(data<span style="color: #666666">=</span>correlation_matrix, annot<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="discussing-the-correlation-data">Discussing the correlation data </h2>

<p>In the above example we note two things. In the first plot we display
the overlap of benign and malignant tumors as functions of the various
features in the Wisconsing breast cancer data set. We see that for
some of the features we can distinguish clearly the benign and
malignant cases while for other features we cannot. This can point to
us which features may be of greater interest when we wish to classify
a benign or not benign tumour.
</p>

<p>In the second figure we have computed the so-called correlation
matrix, which in our case with thirty features becomes a \( 30\times 30 \)
matrix.
</p>

<p>We constructed this matrix using <b>pandas</b> via the statements</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">cancerpd <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(cancer<span style="color: #666666">.</span>data, columns<span style="color: #666666">=</span>cancer<span style="color: #666666">.</span>feature_names)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>and then</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">correlation_matrix <span style="color: #666666">=</span> cancerpd<span style="color: #666666">.</span>corr()<span style="color: #666666">.</span>round(<span style="color: #666666">1</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Diagonalizing this matrix we can in turn say something about which
features are of relevance and which are not. This leads  us to
the classical Principal Component Analysis (PCA) theorem with
applications. This will be discussed later this semester (<a href="https://compphysics.github.io/MachineLearning/doc/pub/week43/html/week43-bs.html" target="_blank">week 43</a>).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="other-ways-of-presenting-a-classification-problem">Other ways of presenting a classification problem </h2>

<p>For a binary classifcation matrix, the so-called <b>confusion matrix</b>, is often used. It can also be extended to more catgeories/classes as well.
The following quantities are then used
</p>
<ol>
<li> positive condition number \( P \), which represents the number of real positive cases in the data (output one/true etc)</li>
<li> The condition negative number \( N \) which is the number of negative cases (ouput zero/false etc)</li>
<li> The true positive number \( TP \) which represents whether a positive test result has been correctly classified (the application of our trained model on a test data set)</li>
<li> The true negative \( TN \) number which represents whether a negative test has been correctly classified</li>
<li> The false positive \( FP \) number, a so-called type I error which tells us about the fraction of  positive test result which are wrongly classified</li>
<li> A false negative \( FN \) number, a so-called type II error which, should be pretty obvious, indicates if a negative test has been wrongly classified.</li>
</ol>
<p>It is is easy to think in terms of illness. You could think of the above as</p>
<ol>
<li> True positive: Sick people correctly identified as sick</li>
<li> False positive: Healthy people incorrectly identified as sick</li>
<li> True negative: Healthy people correctly identified as healthy</li>
<li> False negative: Sick people incorrectly identified as healthy</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="combinations-of-classification-results">Combinations of classification results </h2>

<p>It is common in the literature to define various combinations the above numbers. The most commonly used are</p>

<div class="alert alert-block alert-block alert-text-normal">
<b>Sensitivity, recall, hit rate, or true positive rate \( TPR \). It is the probability of a positive test result, conditioned on the individual truly being positive.</b>
<p>
$$
{\displaystyle \mathrm {TPR} ={\frac {\mathrm {TP} }{\mathrm {P} }}={\frac {\mathrm {TP} }{\mathrm {TP} +\mathrm {FN} }}=1-\mathrm {FNR} }
$$
</div>

<p>The \( TPR \) defines how many correct positive results occur among all positive samples available during the test</p>

<div class="alert alert-block alert-block alert-text-normal">
<b>Miss rate or false negative rate \( FNR \)</b>
<p>
$$
{\displaystyle \mathrm {FNR} ={\frac {\mathrm {FN} }{\mathrm {P} }}={\frac {\mathrm {FN} }{\mathrm {FN} +\mathrm {TP} }} }
$$
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b>Specificity, selectivity or true negative rate \( TNR \). It is the probability of a negative test result, conditioned on the individual truly being negative.</b>
<p>
$$
{\displaystyle \mathrm {TNR} ={\frac {\mathrm {TN} }{\mathrm {N} }}={\frac {\mathrm {TN} }{\mathrm {TN} +\mathrm {FP} }}=1-\mathrm {FPR} }
$$

<p>with the fall-out false positive rate</p>
$$
{\displaystyle \mathrm {FPR} ={\frac {\mathrm {FP} }{\mathrm {N} }}={\frac {\mathrm {FP} }{\mathrm {FP} +\mathrm {TN} }}=1-\mathrm {TNR} }
$$
</div>

<p>The \( FPR \) defines how many incorrect positive results occur among
all negative samples available during the test.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="positive-and-negative-prediction-values">Positive and negative prediction values </h2>

<p>The positive and negative predictive values 
are the proportions of positive and negative results in statistics and
diagnostic tests that are true positive and true negative results,
respectively.[1] The PPV and NPV describe the performance of a
diagnostic test or other statistical measure. A high result can be
interpreted as indicating the accuracy of such a statistic.
</p>

<div class="alert alert-block alert-block alert-text-normal">
<b>Precision or positive predictive value \( PPV \).</b>
<p>
$$
{\displaystyle \mathrm {PPV} ={\frac {\mathrm {TP} }{\mathrm {TP} +\mathrm {FP} }}=1-\mathrm {FDR} }
$$
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b>Negative predictive value \( NPV \)</b>
<p>
$$
{\displaystyle \mathrm {NPV} ={\frac {\mathrm {TN} }{\mathrm {TN} +\mathrm {FN} }}=1-\mathrm {FOR} }
$$
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="other-quantities">Other quantities </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>False discovery rate \( FDR \)</b>
<p>
$$
{\displaystyle \mathrm {FDR} ={\frac {\mathrm {FP} }{\mathrm {FP} +\mathrm {TP} }}=1-\mathrm {PPV} }
$$
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b>False omission rate \( FOR \)</b>
<p>
$$
{\displaystyle \mathrm {FOR} ={\frac {\mathrm {FN} }{\mathrm {FN} +\mathrm {TN} }}=1-\mathrm {NPV} }
$$
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="f-1-score">\( F_1 \) score </h2>

<p>In statistical analysis of binary classification, the F-score or
F-measure is a measure of a test's accuracy. It is calculated from the
precision and recall of the test, where the precision is the number of
true positive results divided by the number of all positive results,
including those not identified correctly, and the recall is the number
of true positive results divided by the number of all samples that
should have been identified as positive. Precision is also known as
positive predictive value, and recall is also known as sensitivity in
diagnostic binary classification.
</p>

<p>The F1 score is the harmonic mean of the precision and recall. It thus
symmetrically represents both precision and recall in one metric.  The
highest possible value of an F-score is 1.0, indicating perfect
precision and recall, and the lowest possible value is 0, if either
precision or recall are zero.
</p>

<p>It is defined as </p>
$$
{\displaystyle \mathrm {F} _{1}=2\times {\frac {\mathrm {PPV} \times \mathrm {TPR} }{\mathrm {PPV} +\mathrm {TPR} }}={\frac {2\mathrm {TP} }{2\mathrm {TP} +\mathrm {FP} +\mathrm {FN} }}}
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="roc-curve">ROC curve </h2>

<p>A receiver operating characteristic curve, or ROC curve, is a
graphical plot that illustrates the performance of a binary classifier
model at varying threshold values.
</p>

<p>The ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting.</p>

<p>To draw a ROC curve, only the true positive rate (TPR) and false
positive rate (FPR) are needed (as functions of some classifier
parameter). The TPR defines how many correct positive results occur
among all positive samples available during the test. FPR, on the
other hand, defines how many incorrect positive results occur among
all negative samples available during the test.
</p>

<p>See <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank"><tt>https://en.wikipedia.org/wiki/Receiver_operating_characteristic</tt></a> for more discussions.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="cumulative-gain-curve">Cumulative gain curve </h2>

<p>The cumulative gain curve is a performance evaluation used typically for binary classification problems.
It plots the \( TPR \) True Positive Rate or Sensitivity (which represents the 
fraction of examples correctly classified
against Predictive Positive Rate, which represents 
the fraction of positively predicted examples.
</p>

<p>The examples below show the confusion matrix, the ROC curve and the cumulative gain for the Wisconsin cancer data.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="other-measures-in-classification-studies-cancer-data-again">Other measures in classification studies: Cancer Data  again </h2>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span>  train_test_split 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.datasets</span> <span style="color: #008000; font-weight: bold">import</span> load_breast_cancer
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LogisticRegression

<span style="color: #408080; font-style: italic"># Load the data</span>
cancer <span style="color: #666666">=</span> load_breast_cancer()

X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(cancer<span style="color: #666666">.</span>data,cancer<span style="color: #666666">.</span>target,random_state<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(X_train<span style="color: #666666">.</span>shape)
<span style="color: #008000">print</span>(X_test<span style="color: #666666">.</span>shape)
<span style="color: #408080; font-style: italic"># Logistic Regression</span>
logreg <span style="color: #666666">=</span> LogisticRegression(solver<span style="color: #666666">=</span><span style="color: #BA2121">&#39;lbfgs&#39;</span>)
logreg<span style="color: #666666">.</span>fit(X_train, y_train)

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> LabelEncoder
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> cross_validate
<span style="color: #408080; font-style: italic">#Cross validation</span>
accuracy <span style="color: #666666">=</span> cross_validate(logreg,X_test,y_test,cv<span style="color: #666666">=10</span>)[<span style="color: #BA2121">&#39;test_score&#39;</span>]
<span style="color: #008000">print</span>(accuracy)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Test set accuracy with Logistic Regression: </span><span style="color: #BB6688; font-weight: bold">{:.2f}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(logreg<span style="color: #666666">.</span>score(X_test,y_test)))

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">scikitplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skplt</span>
y_pred <span style="color: #666666">=</span> logreg<span style="color: #666666">.</span>predict(X_test)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_confusion_matrix(y_test, y_pred, normalize<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
plt<span style="color: #666666">.</span>show()
y_probas <span style="color: #666666">=</span> logreg<span style="color: #666666">.</span>predict_proba(X_test)
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_roc(y_test, y_probas)
plt<span style="color: #666666">.</span>show()
skplt<span style="color: #666666">.</span>metrics<span style="color: #666666">.</span>plot_cumulative_gain(y_test, y_probas)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="material-for-lecture-thursday-november-9">Material for Lecture Thursday November 9 </h2>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="recurrent-neural-networks-rnns-overarching-view">Recurrent neural networks (RNNs): Overarching view </h2>

<p>Till now our focus has been, including convolutional neural networks
as well, on feedforward neural networks. The output or the activations
flow only in one direction, from the input layer to the output layer.
</p>

<p>A recurrent neural network (RNN) looks very much like a feedforward
neural network, except that it also has connections pointing
backward. 
</p>

<p>RNNs are used to analyze time series data such as stock prices, and
tell you when to buy or sell. In autonomous driving systems, they can
anticipate car trajectories and help avoid accidents. More generally,
they can work on sequences of arbitrary lengths, rather than on
fixed-sized inputs like all the nets we have discussed so far. For
example, they can take sentences, documents, or audio samples as
input, making them extremely useful for natural language processing
systems such as automatic translation and speech-to-text.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="a-simple-example">A simple example </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># Start importing packages</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> datasets, layers, models
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> Input
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.models</span> <span style="color: #008000; font-weight: bold">import</span> Model, Sequential 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> Dense, SimpleRNN, LSTM, GRU
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> optimizers     
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> regularizers           
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.utils</span> <span style="color: #008000; font-weight: bold">import</span> to_categorical 



<span style="color: #408080; font-style: italic"># convert into dataset matrix</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">convertToMatrix</span>(data, step):
 X, Y <span style="color: #666666">=</span>[], []
 <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data)<span style="color: #666666">-</span>step):
  d<span style="color: #666666">=</span>i<span style="color: #666666">+</span>step  
  X<span style="color: #666666">.</span>append(data[i:d,])
  Y<span style="color: #666666">.</span>append(data[d,])
 <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>array(X), np<span style="color: #666666">.</span>array(Y)

step <span style="color: #666666">=</span> <span style="color: #666666">4</span>
N <span style="color: #666666">=</span> <span style="color: #666666">1000</span>    
Tp <span style="color: #666666">=</span> <span style="color: #666666">800</span>    

t<span style="color: #666666">=</span>np<span style="color: #666666">.</span>arange(<span style="color: #666666">0</span>,N)
x<span style="color: #666666">=</span>np<span style="color: #666666">.</span>sin(<span style="color: #666666">0.02*</span>t)<span style="color: #666666">+2*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(N)
df <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(x)
df<span style="color: #666666">.</span>head()

values<span style="color: #666666">=</span>df<span style="color: #666666">.</span>values
train,test <span style="color: #666666">=</span> values[<span style="color: #666666">0</span>:Tp,:], values[Tp:N,:]

<span style="color: #408080; font-style: italic"># add step elements into train and test</span>
test <span style="color: #666666">=</span> np<span style="color: #666666">.</span>append(test,np<span style="color: #666666">.</span>repeat(test[<span style="color: #666666">-1</span>,],step))
train <span style="color: #666666">=</span> np<span style="color: #666666">.</span>append(train,np<span style="color: #666666">.</span>repeat(train[<span style="color: #666666">-1</span>,],step))
 
trainX,trainY <span style="color: #666666">=</span>convertToMatrix(train,step)
testX,testY <span style="color: #666666">=</span>convertToMatrix(test,step)
trainX <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape(trainX, (trainX<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>], <span style="color: #666666">1</span>, trainX<span style="color: #666666">.</span>shape[<span style="color: #666666">1</span>]))
testX <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape(testX, (testX<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>], <span style="color: #666666">1</span>, testX<span style="color: #666666">.</span>shape[<span style="color: #666666">1</span>]))

model <span style="color: #666666">=</span> Sequential()
model<span style="color: #666666">.</span>add(SimpleRNN(units<span style="color: #666666">=32</span>, input_shape<span style="color: #666666">=</span>(<span style="color: #666666">1</span>,step), activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>))
model<span style="color: #666666">.</span>add(Dense(<span style="color: #666666">8</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;relu&quot;</span>)) 
model<span style="color: #666666">.</span>add(Dense(<span style="color: #666666">1</span>))
model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;mean_squared_error&#39;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;rmsprop&#39;</span>)
model<span style="color: #666666">.</span>summary()

model<span style="color: #666666">.</span>fit(trainX,trainY, epochs<span style="color: #666666">=100</span>, batch_size<span style="color: #666666">=16</span>, verbose<span style="color: #666666">=2</span>)
trainPredict <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(trainX)
testPredict<span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(testX)
predicted<span style="color: #666666">=</span>np<span style="color: #666666">.</span>concatenate((trainPredict,testPredict),axis<span style="color: #666666">=0</span>)

trainScore <span style="color: #666666">=</span> model<span style="color: #666666">.</span>evaluate(trainX, trainY, verbose<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(trainScore)
plt<span style="color: #666666">.</span>plot(df)
plt<span style="color: #666666">.</span>plot(predicted)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h3 id="rnns">RNNs </h3>

<p>RNNs are very powerful, because they
combine two properties:
</p>
<ol>
<li> Distributed hidden state that allows them to store a lot of information about the past efficiently.</li>
<li> Non-linear dynamics that allows them to update their hidden state in complicated ways.</li>
</ol>
<p>With enough neurons and time, RNNs
can compute anything that can be
computed by your computer!
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basic-layout">Basic layout </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN1.png" width="700" align="bottom"></p>
</center>
<br/><br/>
<h3 id="we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units">We need to specify the initial activity state of all the hidden and output units </h3>

<ol>
<li> We could just fix these initial states to have some default value like 0.5.</li>
<li> But it is better to treat the initial states as learned parameters.</li>
<li> We learn them in the same way as we learn the weights.</li>
<li> Start off with an initial random guess for the initial states.
<ol type="a"></li>
 <li> At the end of each training sequence, backpropagate through time all the way to the initial states to get the gradient of the error function with respect to each initial state.</li>
 <li> Adjust the initial states by following the negative gradient.</li> 
</ol>
</ol>
<h3 id="we-can-specify-inputs-in-several-ways">We can specify inputs in several ways </h3>

<ol>
<li> Specify the initial states of all the units.</li>
<li> Specify the initial states of a subset of the units.</li>
<li> Specify the states of the same subset of the units at every time step.</li>
</ol>
<p>This is the natural way to model most sequential data. </p>
<h3 id="we-can-specify-targets-in-several-ways">We can specify targets in several ways </h3>

<ol>
<li> Specify desired final activities of all the units</li>
<li> Specify desired activities of all units for the last few steps</li>
<li> Good for learning attractors</li>
<li> It is easy to add in extra error derivatives as we backpropagate.</li>
<ul>
  <li> Specify the desired activity of a subset of the units.</li>
</ul>
<li> The other units are input or hidden units.</li> 
</ol>
<br/><br/>
<center>
<p><img src="figslides/RNN2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN3.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN4.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN5.png" width="700" align="bottom"></p>
</center>
<br/><br/>
<h3 id="backpropagation-through-time">Backpropagation through time </h3>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>We can think of the recurrent net as a layered, feed-forward
net with shared weights and then train the feed-forward net
with weight constraints.
</p>
</div>


<p>We can also think of this training algorithm in the time domain:</p>
<ol>
<li> The forward pass builds up a stack of the activities of all the units at each time step.</li>
<li> The backward pass peels activities off the stack to compute the error derivatives at each time step.</li>
<li> After the backward pass we add together the derivatives at all the different times for each weight.</li> 
</ol>
<h3 id="the-backward-pass-is-linear">The backward pass is linear </h3>

<ol>
<li> There is a big difference between the forward and backward passes.</li>
<li> In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.</li>
<li> The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.</li>
</ol>
<p>The forward pass determines the slope of the linear function used for
backpropagating through each neuron
</p>

<br/><br/>
<center>
<p><img src="figslides/RNN6.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN7.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN8.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN9.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN10.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN11.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN12.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split  -->
<h2 id="the-problem-of-exploding-or-vanishing-gradients">The problem of exploding or vanishing gradients </h2>
<ul>
<li> What happens to the magnitude of the gradients as we backpropagate through many layers?
<ol type="a"></li>
 <li> If the weights are small, the gradients shrink exponentially.</li>
 <li> If the weights are big the gradients grow exponentially.</li>
</ol>
<li> Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.</li>
<li> In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.
<ol type="a"></li>
 <li> We can avoid this by initializing the weights very carefully.</li>
</ol>
<li> Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.</li>
</ul>
<p>RNNs have difficulty dealing with long-range dependencies. </p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="four-effective-ways-to-learn-an-rnn">Four effective ways to learn an RNN </h2>
<ol>
<li> Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.</li>
<li> Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.</li>
<li> Echo State Networks: Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.</li>
<ul>
  <li> ESNs only need to learn the hidden-output connections.</li>
</ul>
<li> Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum</li>
</ol>
<h3 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM) </h3>

<p>LSTM uses a memory cell for 
 modeling long-range dependencies and avoid vanishing gradient
 problems.
</p>

<ol>
<li> Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).</li>
<li> They designed a memory cell using logistic and linear units with multiplicative interactions.</li>
<li> Information gets into the cell whenever its &#8220;write&#8221; gate is on.</li>
<li> The information stays in the cell so long as its <b>keep</b> gate is on.</li>
<li> Information can be read from the cell by turning on its <b>read</b> gate.</li> 
</ol>
<h3 id="implementing-a-memory-cell-in-a-neural-network">Implementing a memory cell in a neural network </h3>
<p>To preserve information for a long time in
the activities of an RNN, we use a circuit
that implements an analog memory cell.
</p>

<ol>
<li> A linear unit that has a self-link with a weight of 1 will maintain its state.</li>
<li> Information is stored in the cell by activating its write gate.</li>
<li> Information is retrieved by activating the read gate.</li>
<li> We can backpropagate through this circuit because logistics are have nice derivatives.</li> 
</ol>
<br/><br/>
<center>
<p><img src="figslides/RNN13.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN14.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN15.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN16.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN17.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN18.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN19.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN20.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN21.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN22.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="an-extrapolation-example">An extrapolation example </h2>

<p>The following code provides an example of how recurrent neural
networks can be used to extrapolate to unknown values of physics data
sets.  Specifically, the data sets used in this program come from
a quantum mechanical many-body calculation of energies as functions of the number of particles.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># For matrices and calculations</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #408080; font-style: italic"># For machine learning (backend for keras)</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #408080; font-style: italic"># User-friendly machine learning library</span>
<span style="color: #408080; font-style: italic"># Front end for TensorFlow</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span>
<span style="color: #408080; font-style: italic"># Different methods from Keras needed to create an RNN</span>
<span style="color: #408080; font-style: italic"># This is not necessary but it shortened function calls </span>
<span style="color: #408080; font-style: italic"># that need to be used in the code.</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> datasets, layers, models
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> Input
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> regularizers
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.models</span> <span style="color: #008000; font-weight: bold">import</span> Model, Sequential
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> Dense, SimpleRNN, LSTM, GRU
<span style="color: #408080; font-style: italic"># For timing the code</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">timeit</span> <span style="color: #008000; font-weight: bold">import</span> default_timer <span style="color: #008000; font-weight: bold">as</span> timer
<span style="color: #408080; font-style: italic"># For plotting</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>


<span style="color: #408080; font-style: italic"># The data set</span>
datatype<span style="color: #666666">=</span><span style="color: #BA2121">&#39;VaryDimension&#39;</span>
X_tot <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">2</span>, <span style="color: #666666">42</span>, <span style="color: #666666">2</span>)
y_tot <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([<span style="color: #666666">-0.03077640549</span>, <span style="color: #666666">-0.08336233266</span>, <span style="color: #666666">-0.1446729567</span>, <span style="color: #666666">-0.2116753732</span>, <span style="color: #666666">-0.2830637392</span>, <span style="color: #666666">-0.3581341341</span>, <span style="color: #666666">-0.436462435</span>, <span style="color: #666666">-0.5177783846</span>,
	<span style="color: #666666">-0.6019067271</span>, <span style="color: #666666">-0.6887363571</span>, <span style="color: #666666">-0.7782028952</span>, <span style="color: #666666">-0.8702784034</span>, <span style="color: #666666">-0.9649652536</span>, <span style="color: #666666">-1.062292565</span>, <span style="color: #666666">-1.16231451</span>, 
	<span style="color: #666666">-1.265109911</span>, <span style="color: #666666">-1.370782966</span>, <span style="color: #666666">-1.479465113</span>, <span style="color: #666666">-1.591317992</span>, <span style="color: #666666">-1.70653767</span>])
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="formatting-the-data">Formatting the Data </h2>

<p>The way the recurrent neural networks are trained in this program
differs from how machine learning algorithms are usually trained.
Typically a machine learning algorithm is trained by learning the
relationship between the x data and the y data.  In this program, the
recurrent neural network will be trained to recognize the relationship
in a sequence of y values.  This is type of data formatting is
typically used time series forcasting, but it can also be used in any
extrapolation (time series forecasting is just a specific type of
extrapolation along the time axis).  This method of data formatting
does not use the x data and assumes that the y data are evenly spaced.
</p>

<p>For a standard machine learning algorithm, the training data has the
form of (x,y) so the machine learning algorithm learns to assiciate a
y value with a given x value.  This is useful when the test data has x
values within the same range as the training data.  However, for this
application, the x values of the test data are outside of the x values
of the training data and the traditional method of training a machine
learning algorithm does not work as well.  For this reason, the
recurrent neural network is trained on sequences of y values of the
form ((y1, y2), y3), so that the network is concerned with learning
the pattern of the y data and not the relation between the x and y
data.  As long as the pattern of y data outside of the training region
stays relatively stable compared to what was inside the training
region, this method of training can produce accurate extrapolations to
y values far removed from the training data set.
</p>

<!--  -->
<!-- The idea behind formatting the data in this way comes from [this resource](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) and [this one](https://fairyonice.github.io/Understand-Keras%27s-RNN-behind-the-scenes-with-a-sin-wave-example.html). -->
<!--  -->
<!-- The following method takes in a y data set and formats it so the "x data" are of the form (y1, y2) and the "y data" are of the form y3, with extra brackets added in to make the resulting arrays compatable with both Keras and Tensorflow. -->
<!--  -->
<!-- Note: Using a sequence length of two is not required for time series forecasting so any lenght of sequence could be used (for example instead of ((y1, y2) y3) you could change the length of sequence to be 4 and the resulting data points would have the form ((y1, y2, y3, y4), y5)).  While the following method can be used to create a data set of any sequence length, the remainder of the code expects the length of sequence to be 2.  This is because the data sets are very small and the higher the lenght of the sequence the less resulting data points. -->


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># FORMAT_DATA</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">format_data</span>(data, length_of_sequence <span style="color: #666666">=</span> <span style="color: #666666">2</span>):  
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            data(a numpy array): the data that will be the inputs to the recurrent neural</span>
<span style="color: #BA2121; font-style: italic">                network</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequence (an int): the number of elements in one iteration of the</span>
<span style="color: #BA2121; font-style: italic">                sequence patter.  For a function approximator use length_of_sequence = 2.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            rnn_input (a 3D numpy array): the input data for the recurrent neural network.  Its</span>
<span style="color: #BA2121; font-style: italic">                dimensions are length of data - length of sequence, length of sequence, </span>
<span style="color: #BA2121; font-style: italic">                dimnsion of data</span>
<span style="color: #BA2121; font-style: italic">            rnn_output (a numpy array): the training data for the neural network</span>
<span style="color: #BA2121; font-style: italic">        Formats data to be used in a recurrent neural network.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    X, Y <span style="color: #666666">=</span> [], []
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data)<span style="color: #666666">-</span>length_of_sequence):
        <span style="color: #408080; font-style: italic"># Get the next length_of_sequence elements</span>
        a <span style="color: #666666">=</span> data[i:i<span style="color: #666666">+</span>length_of_sequence]
        <span style="color: #408080; font-style: italic"># Get the element that immediately follows that</span>
        b <span style="color: #666666">=</span> data[i<span style="color: #666666">+</span>length_of_sequence]
        <span style="color: #408080; font-style: italic"># Reshape so that each data point is contained in its own array</span>
        a <span style="color: #666666">=</span> np<span style="color: #666666">.</span>reshape (a, (<span style="color: #008000">len</span>(a), <span style="color: #666666">1</span>))
        X<span style="color: #666666">.</span>append(a)
        Y<span style="color: #666666">.</span>append(b)
    rnn_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)
    rnn_output <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(Y)

    <span style="color: #008000; font-weight: bold">return</span> rnn_input, rnn_output


<span style="color: #408080; font-style: italic"># ## Defining the Recurrent Neural Network Using Keras</span>
<span style="color: #408080; font-style: italic"># </span>
<span style="color: #408080; font-style: italic"># The following method defines a simple recurrent neural network in keras consisting of one input layer, one hidden layer, and one output layer.</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">rnn</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with one hidden layer and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the input and output layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the hidden layer</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">200</span>
    <span style="color: #408080; font-style: italic"># Define the input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons))  
    <span style="color: #408080; font-style: italic"># Define the hidden layer as a simple RNN layer with a set number of neurons and add it to </span>
    <span style="color: #408080; font-style: italic"># the network immediately after the input layer</span>
    rnn <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN&quot;</span>)(inp)
    <span style="color: #408080; font-style: italic"># Define the output layer as a dense neural network layer (standard neural network layer)</span>
    <span style="color: #408080; font-style: italic">#and add it to the network immediately after the hidden layer.</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn)
    <span style="color: #408080; font-style: italic"># Create the machine learning model starting with the input layer and ending with the </span>
    <span style="color: #408080; font-style: italic"># output layer</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the machine learning model using the mean squared error function as the loss </span>
    <span style="color: #408080; font-style: italic"># function and an Adams optimizer.</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&quot;mean_squared_error&quot;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&quot;adam&quot;</span>)  
    <span style="color: #008000; font-weight: bold">return</span> model
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="predicting-new-points-with-a-trained-recurrent-neural-network">Predicting New Points With A Trained Recurrent Neural Network </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">test_rnn</span> (x1, y_test, plot_min, plot_max):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            x1 (a list or numpy array): The complete x component of the data set</span>
<span style="color: #BA2121; font-style: italic">            y_test (a list or numpy array): The complete y component of the data set</span>
<span style="color: #BA2121; font-style: italic">            plot_min (an int or float): the smallest x value used in the training data</span>
<span style="color: #BA2121; font-style: italic">            plot_max (an int or float): the largest x valye used in the training data</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            None.</span>
<span style="color: #BA2121; font-style: italic">        Uses a trained recurrent neural network model to predict future points in the </span>
<span style="color: #BA2121; font-style: italic">        series.  Computes the MSE of the predicted data set from the true data set, saves</span>
<span style="color: #BA2121; font-style: italic">        the predicted data set to a csv file, and plots the predicted and true data sets w</span>
<span style="color: #BA2121; font-style: italic">        while also displaying the data range used for training.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Add the training data as the first dim points in the predicted data array as these</span>
    <span style="color: #408080; font-style: italic"># are known values.</span>
    y_pred <span style="color: #666666">=</span> y_test[:dim]<span style="color: #666666">.</span>tolist()
    <span style="color: #408080; font-style: italic"># Generate the first input to the trained recurrent neural network using the last two </span>
    <span style="color: #408080; font-style: italic"># points of the training data.  Based on how the network was trained this means that it</span>
    <span style="color: #408080; font-style: italic"># will predict the first point in the data set after the training data.  All of the </span>
    <span style="color: #408080; font-style: italic"># brackets are necessary for Tensorflow.</span>
    next_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[[y_test[dim<span style="color: #666666">-2</span>]], [y_test[dim<span style="color: #666666">-1</span>]]]])
    <span style="color: #408080; font-style: italic"># Save the very last point in the training data set.  This will be used later.</span>
    last <span style="color: #666666">=</span> [y_test[dim<span style="color: #666666">-1</span>]]

    <span style="color: #408080; font-style: italic"># Iterate until the complete data set is created.</span>
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span> (dim, <span style="color: #008000">len</span>(y_test)):
        <span style="color: #408080; font-style: italic"># Predict the next point in the data set using the previous two points.</span>
        <span style="color: #008000">next</span> <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(next_input)
        <span style="color: #408080; font-style: italic"># Append just the number of the predicted data set</span>
        y_pred<span style="color: #666666">.</span>append(<span style="color: #008000">next</span>[<span style="color: #666666">0</span>][<span style="color: #666666">0</span>])
        <span style="color: #408080; font-style: italic"># Create the input that will be used to predict the next data point in the data set.</span>
        next_input <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[last, <span style="color: #008000">next</span>[<span style="color: #666666">0</span>]]], dtype<span style="color: #666666">=</span>np<span style="color: #666666">.</span>float64)
        last <span style="color: #666666">=</span> <span style="color: #008000">next</span>

    <span style="color: #408080; font-style: italic"># Print the mean squared error between the known data set and the predicted data set.</span>
    <span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;MSE: &#39;</span>, np<span style="color: #666666">.</span>square(np<span style="color: #666666">.</span>subtract(y_test, y_pred))<span style="color: #666666">.</span>mean())
    <span style="color: #408080; font-style: italic"># Save the predicted data set as a csv file for later use</span>
    name <span style="color: #666666">=</span> datatype <span style="color: #666666">+</span> <span style="color: #BA2121">&#39;Predicted&#39;</span><span style="color: #666666">+</span><span style="color: #008000">str</span>(dim)<span style="color: #666666">+</span><span style="color: #BA2121">&#39;.csv&#39;</span>
    np<span style="color: #666666">.</span>savetxt(name, y_pred, delimiter<span style="color: #666666">=</span><span style="color: #BA2121">&#39;,&#39;</span>)
    <span style="color: #408080; font-style: italic"># Plot the known data set and the predicted data set.  The red box represents the region that was used</span>
    <span style="color: #408080; font-style: italic"># for the training data.</span>
    fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
    ax<span style="color: #666666">.</span>plot(x1, y_test, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;true&quot;</span>, linewidth<span style="color: #666666">=3</span>)
    ax<span style="color: #666666">.</span>plot(x1, y_pred, <span style="color: #BA2121">&#39;g-.&#39;</span>,label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;predicted&quot;</span>, linewidth<span style="color: #666666">=4</span>)
    ax<span style="color: #666666">.</span>legend()
    <span style="color: #408080; font-style: italic"># Created a red region to represent the points used in the training data.</span>
    ax<span style="color: #666666">.</span>axvspan(plot_min, plot_max, alpha<span style="color: #666666">=0.25</span>, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;red&#39;</span>)
    plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]


<span style="color: #408080; font-style: italic"># Generate the training data for the RNN, using a sequence of 2</span>
rnn_input, rnn_training <span style="color: #666666">=</span> format_data(y_train, <span style="color: #666666">2</span>)


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
model <span style="color: #666666">=</span> rnn(length_of_sequences <span style="color: #666666">=</span> rnn_input<span style="color: #666666">.</span>shape[<span style="color: #666666">1</span>])
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(rnn_input, rnn_training, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)

<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict more points of the data set</span>
test_rnn(X_tot, y_tot, X_tot[<span style="color: #666666">0</span>], X_tot[dim<span style="color: #666666">-1</span>])
<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="other-things-to-try">Other Things to Try </h2>

<p>Changing the size of the recurrent neural network and its parameters
can drastically change the results you get from the model.  The below
code takes the simple recurrent neural network from above and adds a
second hidden layer, changes the number of neurons in the hidden
layer, and explicitly declares the activation function of the hidden
layers to be a sigmoid function.  The loss function and optimizer can
also be changed but are kept the same as the above network.  These
parameters can be tuned to provide the optimal result from the
network.  For some ideas on how to improve the performance of a
<a href="https://danijar.com/tips-for-training-recurrent-neural-networks" target="_blank">recurrent neural network</a>.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">rnn_2layers</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with two hidden layers and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the input and output layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the hidden layer, increased from the first network</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">500</span>
    <span style="color: #408080; font-style: italic"># Define the input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons))  
    <span style="color: #408080; font-style: italic"># Create two hidden layers instead of one hidden layer.  Explicitly set the activation</span>
    <span style="color: #408080; font-style: italic"># function to be the sigmoid function (the default value is hyperbolic tangent)</span>
    rnn1 <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,  <span style="color: #408080; font-style: italic"># This needs to be True if another hidden layer is to follow</span>
                    stateful <span style="color: #666666">=</span> stateful, activation <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;sigmoid&#39;</span>,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN1&quot;</span>)(inp)
    rnn2 <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>, activation <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;sigmoid&#39;</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN2&quot;</span>)(rnn1)
    <span style="color: #408080; font-style: italic"># Define the output layer as a dense neural network layer (standard neural network layer)</span>
    <span style="color: #408080; font-style: italic">#and add it to the network immediately after the hidden layer.</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn2)
    <span style="color: #408080; font-style: italic"># Create the machine learning model starting with the input layer and ending with the </span>
    <span style="color: #408080; font-style: italic"># output layer</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the machine learning model using the mean squared error function as the loss </span>
    <span style="color: #408080; font-style: italic"># function and an Adams optimizer.</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&quot;mean_squared_error&quot;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&quot;adam&quot;</span>)  
    <span style="color: #008000; font-weight: bold">return</span> model

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]


<span style="color: #408080; font-style: italic"># Generate the training data for the RNN, using a sequence of 2</span>
rnn_input, rnn_training <span style="color: #666666">=</span> format_data(y_train, <span style="color: #666666">2</span>)


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
model <span style="color: #666666">=</span> rnn_2layers(length_of_sequences <span style="color: #666666">=</span> <span style="color: #666666">2</span>)
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(rnn_input, rnn_training, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)


<span style="color: #408080; font-style: italic"># This section plots the training loss and the validation loss as a function of training iteration.</span>
<span style="color: #408080; font-style: italic"># This is not required for analyzing the couple cluster data but can help determine if the network is</span>
<span style="color: #408080; font-style: italic"># being overtrained.</span>
<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict more points of the data set</span>
test_rnn(X_tot, y_tot, X_tot[<span style="color: #666666">0</span>], X_tot[dim<span style="color: #666666">-1</span>])
<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="other-types-of-recurrent-neural-networks">Other Types of Recurrent Neural Networks </h2>

<p>Besides a simple recurrent neural network layer, there are two other
commonly used types of recurrent neural network layers: Long Short
Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short
introduction to these layers see <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_blank"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>
and <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_blank"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>.
</p>

<p>The first network created below is similar to the previous network,
but it replaces the SimpleRNN layers with LSTM layers.  The second
network below has two hidden layers made up of GRUs, which are
preceeded by two dense (feeddorward) neural network layers.  These
dense layers "preprocess" the data before it reaches the recurrent
layers.  This architecture has been shown to improve the performance
of recurrent neural networks (see the link above and also
<a href="https://arxiv.org/pdf/1807.02857.pdf" target="_blank"><tt>https://arxiv.org/pdf/1807.02857.pdf</tt></a>.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">lstm_2layers</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with two LSTM hidden layers and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons on the input/output layer and the number of neurons in the hidden layer</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">250</span>
    <span style="color: #408080; font-style: italic"># Input Layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons)) 
    <span style="color: #408080; font-style: italic"># Hidden layers (in this case they are LSTM layers instead if SimpleRNN layers)</span>
    rnn<span style="color: #666666">=</span> LSTM(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN&quot;</span>, use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;tanh&#39;</span>)(inp)
    rnn1 <span style="color: #666666">=</span> LSTM(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN1&quot;</span>, use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;tanh&#39;</span>)(rnn)
    <span style="color: #408080; font-style: italic"># Output layer</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn1)
    <span style="color: #408080; font-style: italic"># Define the midel</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the model</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;mean_squared_error&#39;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;adam&#39;</span>)  
    <span style="color: #408080; font-style: italic"># Return the model</span>
    <span style="color: #008000; font-weight: bold">return</span> model

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">dnn2_gru2</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with four hidden layers (two dense followed by</span>
<span style="color: #BA2121; font-style: italic">        two GRU layers) and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>    
    <span style="color: #408080; font-style: italic"># Number of neurons on the input/output layers and hidden layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">250</span>
    <span style="color: #408080; font-style: italic"># Input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons)) 
    <span style="color: #408080; font-style: italic"># Hidden Dense (feedforward) layers</span>
    dnn <span style="color: #666666">=</span> Dense(hidden_neurons<span style="color: #666666">/2</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>, name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;dnn&#39;</span>)(inp)
    dnn1 <span style="color: #666666">=</span> Dense(hidden_neurons<span style="color: #666666">/2</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>, name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;dnn1&#39;</span>)(dnn)
    <span style="color: #408080; font-style: italic"># Hidden GRU layers</span>
    rnn1 <span style="color: #666666">=</span> GRU(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN1&quot;</span>, use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)(dnn1)
    rnn <span style="color: #666666">=</span> GRU(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN&quot;</span>, use_bias<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)(rnn1)
    <span style="color: #408080; font-style: italic"># Output layer</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn)
    <span style="color: #408080; font-style: italic"># Define the model</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the mdoel</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;mean_squared_error&#39;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;adam&#39;</span>)  
    <span style="color: #408080; font-style: italic"># Return the model</span>
    <span style="color: #008000; font-weight: bold">return</span> model

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]


<span style="color: #408080; font-style: italic"># Generate the training data for the RNN, using a sequence of 2</span>
rnn_input, rnn_training <span style="color: #666666">=</span> format_data(y_train, <span style="color: #666666">2</span>)


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
<span style="color: #408080; font-style: italic"># Change the method name to reflect which network you want to use</span>
model <span style="color: #666666">=</span> dnn2_gru2(length_of_sequences <span style="color: #666666">=</span> <span style="color: #666666">2</span>)
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(rnn_input, rnn_training, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)


<span style="color: #408080; font-style: italic"># This section plots the training loss and the validation loss as a function of training iteration.</span>
<span style="color: #408080; font-style: italic"># This is not required for analyzing the couple cluster data but can help determine if the network is</span>
<span style="color: #408080; font-style: italic"># being overtrained.</span>
<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict more points of the data set</span>
test_rnn(X_tot, y_tot, X_tot[<span style="color: #666666">0</span>], X_tot[dim<span style="color: #666666">-1</span>])
<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)


<span style="color: #408080; font-style: italic"># ### Training Recurrent Neural Networks in the Standard Way (i.e. learning the relationship between the X and Y data)</span>
<span style="color: #408080; font-style: italic"># </span>
<span style="color: #408080; font-style: italic"># Finally, comparing the performace of a recurrent neural network using the standard data formatting to the performance of the network with time sequence data formatting shows the benefit of this type of data formatting with extrapolation.</span>

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]

<span style="color: #408080; font-style: italic"># Reshape the data for Keras specifications</span>
X_train <span style="color: #666666">=</span> X_train<span style="color: #666666">.</span>reshape((dim, <span style="color: #666666">1</span>))
y_train <span style="color: #666666">=</span> y_train<span style="color: #666666">.</span>reshape((dim, <span style="color: #666666">1</span>))


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
<span style="color: #408080; font-style: italic"># Set the sequence length to 1 for regular data formatting </span>
model <span style="color: #666666">=</span> rnn(length_of_sequences <span style="color: #666666">=</span> <span style="color: #666666">1</span>)
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(X_train, y_train, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)


<span style="color: #408080; font-style: italic"># This section plots the training loss and the validation loss as a function of training iteration.</span>
<span style="color: #408080; font-style: italic"># This is not required for analyzing the couple cluster data but can help determine if the network is</span>
<span style="color: #408080; font-style: italic"># being overtrained.</span>
<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict the remaining data points</span>
X_pred <span style="color: #666666">=</span> X_tot[dim:]
X_pred <span style="color: #666666">=</span> X_pred<span style="color: #666666">.</span>reshape((<span style="color: #008000">len</span>(X_pred), <span style="color: #666666">1</span>))
y_model <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(X_pred)
y_pred <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate((y_tot[:dim], y_model<span style="color: #666666">.</span>flatten()))

<span style="color: #408080; font-style: italic"># Plot the known data set and the predicted data set.  The red box represents the region that was used</span>
<span style="color: #408080; font-style: italic"># for the training data.</span>
fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
ax<span style="color: #666666">.</span>plot(X_tot, y_tot, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;true&quot;</span>, linewidth<span style="color: #666666">=3</span>)
ax<span style="color: #666666">.</span>plot(X_tot, y_pred, <span style="color: #BA2121">&#39;g-.&#39;</span>,label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;predicted&quot;</span>, linewidth<span style="color: #666666">=4</span>)
ax<span style="color: #666666">.</span>legend()
<span style="color: #408080; font-style: italic"># Created a red region to represent the points used in the training data.</span>
ax<span style="color: #666666">.</span>axvspan(X_tot[<span style="color: #666666">0</span>], X_tot[dim], alpha<span style="color: #666666">=0.25</span>, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;red&#39;</span>)
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- ------------------- end of main content --------------- -->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2023, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

