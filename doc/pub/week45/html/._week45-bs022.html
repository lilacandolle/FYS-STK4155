<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week45.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week45-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 45,  Recurrent Neural Networks">
<title>Week 45,  Recurrent Neural Networks</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week45.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week45-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 45', 2, None, 'plan-for-week-45'),
              ('Material for the lab sessions, additional ways to present '
               'classification results and other practicalities',
               2,
               None,
               'material-for-the-lab-sessions-additional-ways-to-present-classification-results-and-other-practicalities'),
              ('Searching for Optimal Regularization Parameters $\\lambda$',
               2,
               None,
               'searching-for-optimal-regularization-parameters-lambda'),
              ('Grid Search', 2, None, 'grid-search'),
              ('Randomized Grid Search', 2, None, 'randomized-grid-search'),
              ('Wisconsin Cancer Data', 2, None, 'wisconsin-cancer-data'),
              ('Using the correlation matrix',
               2,
               None,
               'using-the-correlation-matrix'),
              ('Discussing the correlation data',
               2,
               None,
               'discussing-the-correlation-data'),
              ('Other ways of presenting a classification problem',
               2,
               None,
               'other-ways-of-presenting-a-classification-problem'),
              ('Combinations of classification results',
               2,
               None,
               'combinations-of-classification-results'),
              ('Positive and negative prediction values',
               2,
               None,
               'positive-and-negative-prediction-values'),
              ('Other quantities', 2, None, 'other-quantities'),
              ('$F_1$ score', 2, None, 'f-1-score'),
              ('ROC curve', 2, None, 'roc-curve'),
              ('Cumulative gain curve', 2, None, 'cumulative-gain-curve'),
              ('Other measures in classification studies: Cancer Data  again',
               2,
               None,
               'other-measures-in-classification-studies-cancer-data-again'),
              ('Material for Lecture Thursday November 9',
               2,
               None,
               'material-for-lecture-thursday-november-9'),
              ('Recurrent neural networks (RNNs): Overarching view',
               2,
               None,
               'recurrent-neural-networks-rnns-overarching-view'),
              ('A simple example', 2, None, 'a-simple-example'),
              ('RNNs', 3, None, 'rnns'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('We need to specify the initial activity state of all the '
               'hidden and output units',
               3,
               None,
               'we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units'),
              ('We can specify inputs in several ways',
               3,
               None,
               'we-can-specify-inputs-in-several-ways'),
              ('We can specify targets in several ways',
               3,
               None,
               'we-can-specify-targets-in-several-ways'),
              ('Backpropagation through time',
               3,
               None,
               'backpropagation-through-time'),
              ('The backward pass is linear',
               3,
               None,
               'the-backward-pass-is-linear'),
              ('The problem of exploding or vanishing gradients',
               2,
               None,
               'the-problem-of-exploding-or-vanishing-gradients'),
              ('Four effective ways to learn an RNN',
               2,
               None,
               'four-effective-ways-to-learn-an-rnn'),
              ('Long Short Term Memory (LSTM)',
               3,
               None,
               'long-short-term-memory-lstm'),
              ('Implementing a memory cell in a neural network',
               3,
               None,
               'implementing-a-memory-cell-in-a-neural-network'),
              ('An extrapolation example', 2, None, 'an-extrapolation-example'),
              ('Formatting the Data', 2, None, 'formatting-the-data'),
              ('Predicting New Points With A Trained Recurrent Neural Network',
               2,
               None,
               'predicting-new-points-with-a-trained-recurrent-neural-network'),
              ('Other Things to Try', 2, None, 'other-things-to-try'),
              ('Other Types of Recurrent Neural Networks',
               2,
               None,
               'other-types-of-recurrent-neural-networks')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week45-bs.html">Week 45,  Recurrent Neural Networks</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week45-bs001.html#plan-for-week-45" style="font-size: 80%;"><b>Plan for week 45</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs002.html#material-for-the-lab-sessions-additional-ways-to-present-classification-results-and-other-practicalities" style="font-size: 80%;"><b>Material for the lab sessions, additional ways to present classification results and other practicalities</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs003.html#searching-for-optimal-regularization-parameters-lambda" style="font-size: 80%;"><b>Searching for Optimal Regularization Parameters \( \lambda \)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs004.html#grid-search" style="font-size: 80%;"><b>Grid Search</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs005.html#randomized-grid-search" style="font-size: 80%;"><b>Randomized Grid Search</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs006.html#wisconsin-cancer-data" style="font-size: 80%;"><b>Wisconsin Cancer Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs007.html#using-the-correlation-matrix" style="font-size: 80%;"><b>Using the correlation matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs008.html#discussing-the-correlation-data" style="font-size: 80%;"><b>Discussing the correlation data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs009.html#other-ways-of-presenting-a-classification-problem" style="font-size: 80%;"><b>Other ways of presenting a classification problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs010.html#combinations-of-classification-results" style="font-size: 80%;"><b>Combinations of classification results</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs011.html#positive-and-negative-prediction-values" style="font-size: 80%;"><b>Positive and negative prediction values</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs012.html#other-quantities" style="font-size: 80%;"><b>Other quantities</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs013.html#f-1-score" style="font-size: 80%;"><b>\( F_1 \) score</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs014.html#roc-curve" style="font-size: 80%;"><b>ROC curve</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs015.html#cumulative-gain-curve" style="font-size: 80%;"><b>Cumulative gain curve</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs016.html#other-measures-in-classification-studies-cancer-data-again" style="font-size: 80%;"><b>Other measures in classification studies: Cancer Data  again</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs017.html#material-for-lecture-thursday-november-9" style="font-size: 80%;"><b>Material for Lecture Thursday November 9</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs018.html#recurrent-neural-networks-rnns-overarching-view" style="font-size: 80%;"><b>Recurrent neural networks (RNNs): Overarching view</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs019.html#a-simple-example" style="font-size: 80%;"><b>A simple example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs019.html#rnns" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;RNNs</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#basic-layout" style="font-size: 80%;"><b>Basic layout</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;We need to specify the initial activity state of all the hidden and output units</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#we-can-specify-inputs-in-several-ways" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;We can specify inputs in several ways</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#we-can-specify-targets-in-several-ways" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;We can specify targets in several ways</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#backpropagation-through-time" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Backpropagation through time</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#the-backward-pass-is-linear" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;The backward pass is linear</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs021.html#the-problem-of-exploding-or-vanishing-gradients" style="font-size: 80%;"><b>The problem of exploding or vanishing gradients</b></a></li>
     <!-- navigation toc: --> <li><a href="#four-effective-ways-to-learn-an-rnn" style="font-size: 80%;"><b>Four effective ways to learn an RNN</b></a></li>
     <!-- navigation toc: --> <li><a href="#long-short-term-memory-lstm" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Long Short Term Memory (LSTM)</a></li>
     <!-- navigation toc: --> <li><a href="#implementing-a-memory-cell-in-a-neural-network" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Implementing a memory cell in a neural network</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs023.html#an-extrapolation-example" style="font-size: 80%;"><b>An extrapolation example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs024.html#formatting-the-data" style="font-size: 80%;"><b>Formatting the Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs025.html#predicting-new-points-with-a-trained-recurrent-neural-network" style="font-size: 80%;"><b>Predicting New Points With A Trained Recurrent Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs026.html#other-things-to-try" style="font-size: 80%;"><b>Other Things to Try</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs027.html#other-types-of-recurrent-neural-networks" style="font-size: 80%;"><b>Other Types of Recurrent Neural Networks</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0022"></a>
<!-- !split -->
<h2 id="four-effective-ways-to-learn-an-rnn" class="anchor">Four effective ways to learn an RNN </h2>
<ol>
<li> Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.</li>
<li> Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.</li>
<li> Echo State Networks: Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.</li>
<ul>
  <li> ESNs only need to learn the hidden-output connections.</li>
</ul>
<li> Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum</li>
</ol>
<h3 id="long-short-term-memory-lstm" class="anchor">Long Short Term Memory (LSTM) </h3>

<p>LSTM uses a memory cell for 
 modeling long-range dependencies and avoid vanishing gradient
 problems.
</p>

<ol>
<li> Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).</li>
<li> They designed a memory cell using logistic and linear units with multiplicative interactions.</li>
<li> Information gets into the cell whenever its &#8220;write&#8221; gate is on.</li>
<li> The information stays in the cell so long as its <b>keep</b> gate is on.</li>
<li> Information can be read from the cell by turning on its <b>read</b> gate.</li> 
</ol>
<h3 id="implementing-a-memory-cell-in-a-neural-network" class="anchor">Implementing a memory cell in a neural network </h3>
<p>To preserve information for a long time in
the activities of an RNN, we use a circuit
that implements an analog memory cell.
</p>

<ol>
<li> A linear unit that has a self-link with a weight of 1 will maintain its state.</li>
<li> Information is stored in the cell by activating its write gate.</li>
<li> Information is retrieved by activating the read gate.</li>
<li> We can backpropagate through this circuit because logistics are have nice derivatives.</li> 
</ol>
<br/><br/>
<center>
<p><img src="figslides/RNN13.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN14.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN15.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN16.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN17.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN18.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN19.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN20.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN21.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<br/><br/>
<center>
<p><img src="figslides/RNN22.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week45-bs021.html">&laquo;</a></li>
  <li><a href="._week45-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week45-bs014.html">15</a></li>
  <li><a href="._week45-bs015.html">16</a></li>
  <li><a href="._week45-bs016.html">17</a></li>
  <li><a href="._week45-bs017.html">18</a></li>
  <li><a href="._week45-bs018.html">19</a></li>
  <li><a href="._week45-bs019.html">20</a></li>
  <li><a href="._week45-bs020.html">21</a></li>
  <li><a href="._week45-bs021.html">22</a></li>
  <li class="active"><a href="._week45-bs022.html">23</a></li>
  <li><a href="._week45-bs023.html">24</a></li>
  <li><a href="._week45-bs024.html">25</a></li>
  <li><a href="._week45-bs025.html">26</a></li>
  <li><a href="._week45-bs026.html">27</a></li>
  <li><a href="._week45-bs027.html">28</a></li>
  <li><a href="._week45-bs023.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>

