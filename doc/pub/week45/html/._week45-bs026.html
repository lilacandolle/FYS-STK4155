<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week45.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week45-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 45,  Recurrent Neural Networks">
<title>Week 45,  Recurrent Neural Networks</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week45.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week45-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 45', 2, None, 'plan-for-week-45'),
              ('Material for the lab sessions, additional ways to present '
               'classification results and other practicalities',
               2,
               None,
               'material-for-the-lab-sessions-additional-ways-to-present-classification-results-and-other-practicalities'),
              ('Searching for Optimal Regularization Parameters $\\lambda$',
               2,
               None,
               'searching-for-optimal-regularization-parameters-lambda'),
              ('Grid Search', 2, None, 'grid-search'),
              ('Randomized Grid Search', 2, None, 'randomized-grid-search'),
              ('Wisconsin Cancer Data', 2, None, 'wisconsin-cancer-data'),
              ('Using the correlation matrix',
               2,
               None,
               'using-the-correlation-matrix'),
              ('Discussing the correlation data',
               2,
               None,
               'discussing-the-correlation-data'),
              ('Other ways of presenting a classification problem',
               2,
               None,
               'other-ways-of-presenting-a-classification-problem'),
              ('Combinations of classification results',
               2,
               None,
               'combinations-of-classification-results'),
              ('Positive and negative prediction values',
               2,
               None,
               'positive-and-negative-prediction-values'),
              ('Other quantities', 2, None, 'other-quantities'),
              ('$F_1$ score', 2, None, 'f-1-score'),
              ('ROC curve', 2, None, 'roc-curve'),
              ('Cumulative gain curve', 2, None, 'cumulative-gain-curve'),
              ('Other measures in classification studies: Cancer Data  again',
               2,
               None,
               'other-measures-in-classification-studies-cancer-data-again'),
              ('Material for Lecture Thursday November 9',
               2,
               None,
               'material-for-lecture-thursday-november-9'),
              ('Recurrent neural networks (RNNs): Overarching view',
               2,
               None,
               'recurrent-neural-networks-rnns-overarching-view'),
              ('A simple example', 2, None, 'a-simple-example'),
              ('RNNs', 3, None, 'rnns'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('We need to specify the initial activity state of all the '
               'hidden and output units',
               3,
               None,
               'we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units'),
              ('We can specify inputs in several ways',
               3,
               None,
               'we-can-specify-inputs-in-several-ways'),
              ('We can specify targets in several ways',
               3,
               None,
               'we-can-specify-targets-in-several-ways'),
              ('Backpropagation through time',
               3,
               None,
               'backpropagation-through-time'),
              ('The backward pass is linear',
               3,
               None,
               'the-backward-pass-is-linear'),
              ('The problem of exploding or vanishing gradients',
               2,
               None,
               'the-problem-of-exploding-or-vanishing-gradients'),
              ('Four effective ways to learn an RNN',
               2,
               None,
               'four-effective-ways-to-learn-an-rnn'),
              ('Long Short Term Memory (LSTM)',
               3,
               None,
               'long-short-term-memory-lstm'),
              ('Implementing a memory cell in a neural network',
               3,
               None,
               'implementing-a-memory-cell-in-a-neural-network'),
              ('An extrapolation example', 2, None, 'an-extrapolation-example'),
              ('Formatting the Data', 2, None, 'formatting-the-data'),
              ('Predicting New Points With A Trained Recurrent Neural Network',
               2,
               None,
               'predicting-new-points-with-a-trained-recurrent-neural-network'),
              ('Other Things to Try', 2, None, 'other-things-to-try'),
              ('Other Types of Recurrent Neural Networks',
               2,
               None,
               'other-types-of-recurrent-neural-networks')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week45-bs.html">Week 45,  Recurrent Neural Networks</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week45-bs001.html#plan-for-week-45" style="font-size: 80%;"><b>Plan for week 45</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs002.html#material-for-the-lab-sessions-additional-ways-to-present-classification-results-and-other-practicalities" style="font-size: 80%;"><b>Material for the lab sessions, additional ways to present classification results and other practicalities</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs003.html#searching-for-optimal-regularization-parameters-lambda" style="font-size: 80%;"><b>Searching for Optimal Regularization Parameters \( \lambda \)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs004.html#grid-search" style="font-size: 80%;"><b>Grid Search</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs005.html#randomized-grid-search" style="font-size: 80%;"><b>Randomized Grid Search</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs006.html#wisconsin-cancer-data" style="font-size: 80%;"><b>Wisconsin Cancer Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs007.html#using-the-correlation-matrix" style="font-size: 80%;"><b>Using the correlation matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs008.html#discussing-the-correlation-data" style="font-size: 80%;"><b>Discussing the correlation data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs009.html#other-ways-of-presenting-a-classification-problem" style="font-size: 80%;"><b>Other ways of presenting a classification problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs010.html#combinations-of-classification-results" style="font-size: 80%;"><b>Combinations of classification results</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs011.html#positive-and-negative-prediction-values" style="font-size: 80%;"><b>Positive and negative prediction values</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs012.html#other-quantities" style="font-size: 80%;"><b>Other quantities</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs013.html#f-1-score" style="font-size: 80%;"><b>\( F_1 \) score</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs014.html#roc-curve" style="font-size: 80%;"><b>ROC curve</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs015.html#cumulative-gain-curve" style="font-size: 80%;"><b>Cumulative gain curve</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs016.html#other-measures-in-classification-studies-cancer-data-again" style="font-size: 80%;"><b>Other measures in classification studies: Cancer Data  again</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs017.html#material-for-lecture-thursday-november-9" style="font-size: 80%;"><b>Material for Lecture Thursday November 9</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs018.html#recurrent-neural-networks-rnns-overarching-view" style="font-size: 80%;"><b>Recurrent neural networks (RNNs): Overarching view</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs019.html#a-simple-example" style="font-size: 80%;"><b>A simple example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs019.html#rnns" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;RNNs</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#basic-layout" style="font-size: 80%;"><b>Basic layout</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#we-need-to-specify-the-initial-activity-state-of-all-the-hidden-and-output-units" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;We need to specify the initial activity state of all the hidden and output units</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#we-can-specify-inputs-in-several-ways" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;We can specify inputs in several ways</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#we-can-specify-targets-in-several-ways" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;We can specify targets in several ways</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#backpropagation-through-time" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Backpropagation through time</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs020.html#the-backward-pass-is-linear" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;The backward pass is linear</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs021.html#the-problem-of-exploding-or-vanishing-gradients" style="font-size: 80%;"><b>The problem of exploding or vanishing gradients</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs022.html#four-effective-ways-to-learn-an-rnn" style="font-size: 80%;"><b>Four effective ways to learn an RNN</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs022.html#long-short-term-memory-lstm" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Long Short Term Memory (LSTM)</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs022.html#implementing-a-memory-cell-in-a-neural-network" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Implementing a memory cell in a neural network</a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs023.html#an-extrapolation-example" style="font-size: 80%;"><b>An extrapolation example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs024.html#formatting-the-data" style="font-size: 80%;"><b>Formatting the Data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs025.html#predicting-new-points-with-a-trained-recurrent-neural-network" style="font-size: 80%;"><b>Predicting New Points With A Trained Recurrent Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="#other-things-to-try" style="font-size: 80%;"><b>Other Things to Try</b></a></li>
     <!-- navigation toc: --> <li><a href="._week45-bs027.html#other-types-of-recurrent-neural-networks" style="font-size: 80%;"><b>Other Types of Recurrent Neural Networks</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0026"></a>
<!-- !split -->
<h2 id="other-things-to-try" class="anchor">Other Things to Try </h2>

<p>Changing the size of the recurrent neural network and its parameters
can drastically change the results you get from the model.  The below
code takes the simple recurrent neural network from above and adds a
second hidden layer, changes the number of neurons in the hidden
layer, and explicitly declares the activation function of the hidden
layers to be a sigmoid function.  The loss function and optimizer can
also be changed but are kept the same as the above network.  These
parameters can be tuned to provide the optimal result from the
network.  For some ideas on how to improve the performance of a
<a href="https://danijar.com/tips-for-training-recurrent-neural-networks" target="_self">recurrent neural network</a>.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">rnn_2layers</span>(length_of_sequences, batch_size <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>, stateful <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Inputs:</span>
<span style="color: #BA2121; font-style: italic">            length_of_sequences (an int): the number of y values in &quot;x data&quot;.  This is determined</span>
<span style="color: #BA2121; font-style: italic">                when the data is formatted</span>
<span style="color: #BA2121; font-style: italic">            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.</span>
<span style="color: #BA2121; font-style: italic">        Returns:</span>
<span style="color: #BA2121; font-style: italic">            model (a Keras model): The recurrent neural network that is built and compiled by this</span>
<span style="color: #BA2121; font-style: italic">                method</span>
<span style="color: #BA2121; font-style: italic">        Builds and compiles a recurrent neural network with two hidden layers and returns the model.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the input and output layers</span>
    in_out_neurons <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    <span style="color: #408080; font-style: italic"># Number of neurons in the hidden layer, increased from the first network</span>
    hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">500</span>
    <span style="color: #408080; font-style: italic"># Define the input layer</span>
    inp <span style="color: #666666">=</span> Input(batch_shape<span style="color: #666666">=</span>(batch_size, 
                length_of_sequences, 
                in_out_neurons))  
    <span style="color: #408080; font-style: italic"># Create two hidden layers instead of one hidden layer.  Explicitly set the activation</span>
    <span style="color: #408080; font-style: italic"># function to be the sigmoid function (the default value is hyperbolic tangent)</span>
    rnn1 <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,  <span style="color: #408080; font-style: italic"># This needs to be True if another hidden layer is to follow</span>
                    stateful <span style="color: #666666">=</span> stateful, activation <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;sigmoid&#39;</span>,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN1&quot;</span>)(inp)
    rnn2 <span style="color: #666666">=</span> SimpleRNN(hidden_neurons, 
                    return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>, activation <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;sigmoid&#39;</span>,
                    stateful <span style="color: #666666">=</span> stateful,
                    name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;RNN2&quot;</span>)(rnn1)
    <span style="color: #408080; font-style: italic"># Define the output layer as a dense neural network layer (standard neural network layer)</span>
    <span style="color: #408080; font-style: italic">#and add it to the network immediately after the hidden layer.</span>
    dens <span style="color: #666666">=</span> Dense(in_out_neurons,name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;dense&quot;</span>)(rnn2)
    <span style="color: #408080; font-style: italic"># Create the machine learning model starting with the input layer and ending with the </span>
    <span style="color: #408080; font-style: italic"># output layer</span>
    model <span style="color: #666666">=</span> Model(inputs<span style="color: #666666">=</span>[inp],outputs<span style="color: #666666">=</span>[dens])
    <span style="color: #408080; font-style: italic"># Compile the machine learning model using the mean squared error function as the loss </span>
    <span style="color: #408080; font-style: italic"># function and an Adams optimizer.</span>
    model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&quot;mean_squared_error&quot;</span>, optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&quot;adam&quot;</span>)  
    <span style="color: #008000; font-weight: bold">return</span> model

<span style="color: #408080; font-style: italic"># Check to make sure the data set is complete</span>
<span style="color: #008000; font-weight: bold">assert</span> <span style="color: #008000">len</span>(X_tot) <span style="color: #666666">==</span> <span style="color: #008000">len</span>(y_tot)

<span style="color: #408080; font-style: italic"># This is the number of points that will be used in as the training data</span>
dim<span style="color: #666666">=12</span>

<span style="color: #408080; font-style: italic"># Separate the training data from the whole data set</span>
X_train <span style="color: #666666">=</span> X_tot[:dim]
y_train <span style="color: #666666">=</span> y_tot[:dim]


<span style="color: #408080; font-style: italic"># Generate the training data for the RNN, using a sequence of 2</span>
rnn_input, rnn_training <span style="color: #666666">=</span> format_data(y_train, <span style="color: #666666">2</span>)


<span style="color: #408080; font-style: italic"># Create a recurrent neural network in Keras and produce a summary of the </span>
<span style="color: #408080; font-style: italic"># machine learning model</span>
model <span style="color: #666666">=</span> rnn_2layers(length_of_sequences <span style="color: #666666">=</span> <span style="color: #666666">2</span>)
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Start the timer.  Want to time training+testing</span>
start <span style="color: #666666">=</span> timer()
<span style="color: #408080; font-style: italic"># Fit the model using the training data genenerated above using 150 training iterations and a 5%</span>
<span style="color: #408080; font-style: italic"># validation split.  Setting verbose to True prints information about each training iteration.</span>
hist <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(rnn_input, rnn_training, batch_size<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, epochs<span style="color: #666666">=150</span>, 
                 verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,validation_split<span style="color: #666666">=0.05</span>)


<span style="color: #408080; font-style: italic"># This section plots the training loss and the validation loss as a function of training iteration.</span>
<span style="color: #408080; font-style: italic"># This is not required for analyzing the couple cluster data but can help determine if the network is</span>
<span style="color: #408080; font-style: italic"># being overtrained.</span>
<span style="color: #008000; font-weight: bold">for</span> label <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&quot;loss&quot;</span>,<span style="color: #BA2121">&quot;val_loss&quot;</span>]:
    plt<span style="color: #666666">.</span>plot(hist<span style="color: #666666">.</span>history[label],label<span style="color: #666666">=</span>label)

plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;loss&quot;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;epoch&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;The final validation loss: </span><span style="color: #BB6688; font-weight: bold">{}</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">.</span>format(hist<span style="color: #666666">.</span>history[<span style="color: #BA2121">&quot;val_loss&quot;</span>][<span style="color: #666666">-1</span>]))
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># Use the trained neural network to predict more points of the data set</span>
test_rnn(X_tot, y_tot, X_tot[<span style="color: #666666">0</span>], X_tot[dim<span style="color: #666666">-1</span>])
<span style="color: #408080; font-style: italic"># Stop the timer and calculate the total time needed.</span>
end <span style="color: #666666">=</span> timer()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Time: &#39;</span>, end<span style="color: #666666">-</span>start)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week45-bs025.html">&laquo;</a></li>
  <li><a href="._week45-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week45-bs018.html">19</a></li>
  <li><a href="._week45-bs019.html">20</a></li>
  <li><a href="._week45-bs020.html">21</a></li>
  <li><a href="._week45-bs021.html">22</a></li>
  <li><a href="._week45-bs022.html">23</a></li>
  <li><a href="._week45-bs023.html">24</a></li>
  <li><a href="._week45-bs024.html">25</a></li>
  <li><a href="._week45-bs025.html">26</a></li>
  <li class="active"><a href="._week45-bs026.html">27</a></li>
  <li><a href="._week45-bs027.html">28</a></li>
  <li><a href="._week45-bs027.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>

